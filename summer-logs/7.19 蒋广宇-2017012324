深度学习的本质（数学）：映射、函数，函数逼近（泛函分析）

万能近似器：以任意的精度来近似任何从一个有限维空间到另一个有限维空间的任意连续函数( Borel可测函数)

训练的本质，就是找到相应的内部权重，使得在训练数据(样本)输 入到网络后，网络的实际输出与预期输出(即标签)之间差异最小。

交叉熵：负对数似然损失函数，主要用于分类任务

反向传播算法：反向逐层计算损失函数对权重的梯度(各个偏导数)

梯度下降法-调节$\theta$数值，使得损失函数变化最快，直到序列收敛

**CNN/RNN 卷积网络/循环网络**

卷积网络：局部区域的权重W共用。循环网络：每一个时间步处理时，权重共享，用于时间序列的预测。

人工神经元 调节权重 多层人工神经网络 

Softmax处理

卷积运算：输入多维数组的数据，卷积核也为多维数组，参数由学习算法得到，数目一般选32、64等

卷积网络基本结构：

1.Convolution/bias/non-linearity activation

2.Normalization layers

2d卷积核、3d卷积核（输入是3d的、有多个卷积核）

循环网络：用于序列建模预测问题。包含训练目标、损失函数、网络输出、状态(隐藏单元) 、网络输入

图灵机：艾伦·图灵提出的抽象计算模，用Sigmoid激活函数的RNN是图灵完备的(Turing-complete)，RNN是图灵完全等价的

双向RNN：结合时间上从序列起点开始移动的RNN 和另 一个时间上从序列末尾开始移动的RNN，能够计算同时依赖于过去和未来且对时刻t的输入值最敏感的表示，用于手写识别和语音识别

语音识别：利用到贝叶斯公式、声学模型、语言模型
