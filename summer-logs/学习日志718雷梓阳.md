#学习日志7/18雷梓阳#

computer system

由计算硬件和软件两部分组成

*个人电脑（单机系统）Xerox Alto是第一台个人电脑

*并行分布式系统（多机系统）超级计算机，云计算系统，大数据系统

**memory hierarchy**

*register/cache

*RAM

*SSD

*Disk

指令集X86与ARM

微架构：使得指令集架构ISA,instruction architecture可以在处理器上被执行

指令集可以在不同的微架构上被执行

处理器的结构设计是研究微架构与指令集两者之间协作的技术

**软件开发**

软件由源码生成

*源代码》预处理器》编译器》汇编器》目标代码》链接器》可执行文件

*机器代码或机器指令

superscalar CPU

*if A then B else C, 同时处理计算三个结果, 提高计算速度

*WORD ：a word is a nature unit of data

**Bitmap indexing for bigdata management**

*inverted indexing

位图索引技术

*a bitmap index is a  collection of bit vectors created for each distinct value in the indexed column

位图索引的四个内容

*bitmap index binning

*bitmap index coding

*bitmap index compression

*bitmap index operations

ffs:在计算机里查找位串中的1的位置，是计算机中的基本指令

bitmap compression 对位串中大量的0进行压缩

*benefit of RLE :good for compression

*downside of RLE :cannot access a random bit

why bitmap index 

*软件编程容易

*经济性好

*系统支持性好

WAH方法，PLWAH方法，压缩数据空间

机器学习的任务

*预测：通过数据集，预测新的数值

*分类：通过数据集，对新数值进行集合分类



- tensorflow的tensor

tensor是tf内部计算的基本数据类型 有Constant，Placeholder，Variable三种表现形式
 --权重需要被修改，权重一般用Variable来表达 --Placeholder一般和计算有关，训练批次几万张图片分成很多批，一次十张，占位符和这个有关
 属性有rank，shape，datatype
 tensorflow代码分成两部分 前半部分用来描述Graph，并不计算（Graph） 后半部分才开始执行计算(Session) 

- Neuron 

--神经元被建模为一个函数σ（Wx+b），其中W是权重，x是输入，b是偏移量 --输入线性加权叠加、 --一个非线性函数σ作用，σ被称为激活函数 --neuron.py 

- loss&GD 

1，定义距离loss   2，GD迭代优化 DNN思考，学习率与收敛问题，xor问题（不是个线性问题，2个relu可解） 随机森林random forest 最近邻方法 

- 人工智能机器智能简述 

设计实现计算机系统软件，使其表现出智能的行为
 机器认知（Cognition）
 --机器学习 --自动推理 --人工意识 --知识表
 机器感知（Perception）
 --语音识别 --视觉识别 --运动控制 

- 机器学习：利用经验集E提升任务T的性能P的方法就是机器学习

deep learning：机器学习的分支
 机器学习的过程1，学习learn 2，推断predict 机器学习可以看成一种数学建模
 目的：推广Generalization 推广误差 Generalization error GE = AE + EE + OE(Approximation error, Estimation error, Optimization error) 误差原因：模型规模，数据集规模，算法设计 echospot
 标志性论文 deep belief nets 2006 AlexNet 2012 “imageNet classification with deep convolutional neural networks” 

- tensorflow主要用来建模神经网络 

neuron 1接受区触发区传导区输出区：接受的线性与触发的非线性 activation function： sigmoid， tanh， ReLU 连接主义方法connectionism：复杂神经网络可以通过大量的简单神经元搭建 多层前馈网络（全连接网络）Multilayer feedforward networks 全连接网络：层间完全二分图（输入层，隐藏层，输出层） 输入和输出看成向量，权重看成矩阵。 深度学习的本质是一个函数逼近问题 万能近似器（Universal Approximators），可以以任意精度近似从一个有限维空间映射到另一个有限维空间的连续函数 

- CNN&RNN    Dense&CNN    学习路线

比如一个4*5*6的tensor --rank(秩)：3d --length：4，5，6 --shape：[4,5,6] --volume: 4*5*6 = 120
 从neuron到layer 单个神经元：输入是1d，参数是1d，输出是0d，一层神经元构成一个layer 输出的shape和layer的shape一致
 batch_size（批尺寸） --会影响输出的shape，不会影响参数的shape 

- 神经网络训练原理 

--采用带标签的训练样本对神经网络进行训练，确定网络的权重参数 ---数据集（x1,y1）（x2,y2）（x3,y3）...... --神经网络输出 ---要求预测值yi'与实际值yi相差较小 --差异最小化 ---度量函数：绝对值求和，平方和，交叉熵
 反向传播算法backprop：根据损失函数的性质以及链式求导法则，反向逐层计算损失函数对权重的梯度（各个偏导数），通过骗到的方向调整权重：梯度下降法（gradient descent） --也称最速下降法，核心是参数值更新（梯度算子），寻找到一个最小值 

- 神经网络的实际训练过程 

--分批训练 --随机梯度下降法（每次只选取一个随机样本）
 整个训练集成为一个批次（batch） 每个迷你批次minibatch的数据被依次送入网络训练，每次训练完被称为“一次迭代”iteration 一个时代（epoch）指训练集中所有的训练样本被送入神经网络，完成一次训练的过程 

- deep learning数值计算方法的实现 

数值计算-数值量化，数值分析
 1，避免两个相近数的相减 2，避免除数远远小于被除数 3，举例：softmax function，当接近0的数被四舍五入时发生下溢underflow 

- 求导运算 

在tensorflow中用的是符号微分 --符号微分原理：1，统一定义基本运算Op2，统一定义初等函数导数运算3，定义四则运算求导法则4，定义导数的链式法则
 学习率就是步长。 计算图CG，tensorflow是一种元编程（meta programming） --sess开始之后才可以开始喂数据 --在图上，由后向前查找相关结点求导插入并更新计算图 