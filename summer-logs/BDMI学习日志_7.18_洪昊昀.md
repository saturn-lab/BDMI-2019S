# BDMI学习日志

作者: 洪昊昀

日期：2019年7月18日

## Abstract

今天首先讲了计算机架构方面的知识点，然后介绍了索引中bitmap方法，再是介绍了Machine Learning 和 Deep Learning，最后进行了TensorFlow的介绍和实验。

## 计算机架构的存储等级

CPU即寄存器是最快的，几乎就是一个时钟周期，但是容量比较小；Cache是高速缓存，速度比registers慢些，但也很快，容量大些；接下来就是内存和硬盘了。可见，计算机的存储硬件基本都有Capacity和Latency的trade-off。

## **Bitmap**

建立索引的方法有：tree/hash/bitmap， 其中bitmap可以解决当查询条件过多（维数过多）导致数据库处理时间很长的问题，它的关键是把集合用位向量表示出来再作交并补。虽然从表面上看索引的位数膨胀了，但其实用位运算表示和整型占的内存单元数量是相当的，计算机底层运算都是位运算，计算机操作时按word操作，占32bit。bitmap受到了计算机体系结构的支持，并且当想寻找字串里哪个是1，用ffs（计算机的基本操作）即可，速度是非常快的。虽然bitmap查找操作灵活，查找速度快，但是它占用的memory较大，此时就需要一些压缩编码技术来解决占用memory过多的问题。

## **机器学习**

利用经验集E提升任务T的性能P的方法就是机器学习 。

机器学习 = 数据+模型+算法
训练：建立统计模型，运用数据，用算法去求解模型参数。
预测：使用训练得到模型参数，预测新的观察事件。

### 深度学习 （深度神经网络）

深度学习是机器学习的一种。它的基本思想是让生物神经元->人工神经元。通过数学建模来模拟生物神经元的工作过程，神经元的模型包括： 接收的线性部分和触发的非线性部分。

具体建模过程如下：

 神经元被建模为一个函数$F(w, x)$，其中$w$是权重， $x$是输入，$ F$称为激活函数
• 输入的线性加权叠加
• 一个非线性函数F作用， 进行输出，
• 激活函数模拟神经元的触发激活特性 

神经元是一些小函数，权重用来刻画变量对判断的重要性。机器学习中有很多可供使用的函数，比如sigmoid函数（logistic函数或S函数）、tanh函数、ReLU函数等。它们分别有不同的数学性质。

#### sigmoid函数

$$sigmoid(x)=\frac{1}{1+e^{-x}}$$

优点：

1. 它的值域为$(0,1)$所以就可以用来作为一种概率的表示。
2. 输出范围是$(0,1)$，输出范围有限，优化稳定，可以用作输出层。
3. 函数光滑，易求导。

缺点：

1. 饱和的时候会导致梯度消失，使训练出现问题。
2. 其输出并不是以0为中心的，据说这样会导致数据因为反向传播时得到的权值产生锯齿形波动，导致收敛速度减慢。

#### tanh函数

$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$

优点：

1. 比$sigmoid$函数收敛速度更快。
2. 输出以0为中心。

缺点：

1. 依然是饱和性导致的梯度消失。

####  ReLU函数

$$relu(x)=max(0,x)$$

优点：

1. 相比起Sigmoid和tanh，ReLU在SGD中能够快速收敛。据说，这是因为它线性、非饱和的形式。
2. .有效缓解了梯度消失的问题。
3. 提供了神经网络的稀疏表达能力。

缺点：

1. 随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。（因为异常输入导致神经元的权重无法更新，永远处于未被触发的关闭状态）。

ReLU函数还有很多变形，是产业中被认为比较好用的几种，如PReLU、LReLU、RReLU等，它们或是让输入小于0时也具有微小的输出，或者是针对不同的输入进行不同的ReLU输出，处理异常值的情况。

### 

## **TensorFlow**

神经网络训练原理：预期输出与实际输出的差异，用损失函数进行刻画，然后转化成损失函数的最小化问题。

回归：用均方误差刻画

分类：用交叉熵函数刻画

### 反向传播算法

反向传播算法反向链式求导法则的具体实现

BP算法(即误差反向传播算法)适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个 输入m输出的BP神经网络所完成的功能是从 维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。 

反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。

BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。

#### 激励传播

每次迭代中的传播环节包含两步：

1. (前向传播阶段)将训练输入送入网络以获得激励响应；
2. (反向传播阶段)将激励响应同训练输入对应的目标输出求差，从而获得隐层和输出层的响应误差。

#### 权重更新

对于每个突触上的权重，按照以下步骤进行更新：

1. 将输入激励和响应误差相乘，从而获得权重的梯度；
2. 将这个梯度乘上一个比例并取反后加到权重上。
3. 这个比例将会影响到训练过程的速度和效果，因此称为“训练因子”。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。

### SGD

随机梯度下降法

batch size影响tensor的维度（即形状），小批量训练是批量训练和随机梯度下降法的一个折中。

### 数值计算：数值量化

google的TensorFlow对数值稳定做了很多操作，ex.无穷大&无穷小没法实现，因为计算机的字长是有限的，

必须对上溢和下溢进行数值稳定。

TensorFlow用的是符号微分的方法进行数值稳定的。:happy:



## Graph & Session

tensor只有3个子类:

1. constant 
2. variables
3. placeholder 占位符

graph即`tf.Graph()`，session即`tf.Session()`

- graph定义了计算方式，是一些加减乘除等运算的组合，类似于一个函数。<u>它本身不会进行任何计算，也不保存任何中间计算结果。</u>它其实就是一个计算图。
- session用来运行一个graph，或者运行graph的一部分。它类似于一个**执行者**，给graph灌入输入数据，得到输出，并保存中间的计算结果。同时它也给graph分配计算资源（如内存、显卡等）。

TensorFlow是一种符号式编程框架，首先要构造一个图（graph），然后在这个图上做运算。打个比方，graph就像一条生产线，session就像生产者。生产线具有一系列的加工步骤（加减乘除等运算），生产者把原料投进去，就能得到产品。不同生产者都可以使用这条生产线，只要他们的加工步骤是一样的就行。同样的，一个graph可以供多个session使用，而一个session不一定需要使用graph的全部，可以只使用其中的一部分。

