卢星宇-2018011280 



7/15

#### **学习内容：安装jupyter notebook，设置git路径，下载课件，初识Python语法，排序 大数乘法**

路径是已经设置好的，插入排序、合并排序，快排以及桶排序是已经熟识的排序算法，也用C++实现过，不知用Python实现会怎么样。复习一下很好，正好多多阅读Python代码。jupyter notebook好像是一种远程桌面，功能才是很强大的，但得一直占着cmd 最有收获的是大数乘法算法的不断改进以及课后那篇最佳算法的推送



7/16

#### **学习内容：红黑树，二叉搜索树，2-3-4树，B+树，哈希查找**。

感觉最奇怪的是红黑树，节点颜色的判定成谜，插入和删除操作有点复杂，好在logn的复杂度很能令人满意，以及完美的平衡效果。2-3-4树是比较陌生的，从来没想过可以在节点里放多个key值，人类的智慧可谓伟大。LLRB树区别于RB树，对边而不是对节点染色，虽然就平衡效果来说，2-3-4树和LLRB树逊色于红黑，但胜在实现方便。今天学到的一个证明是排序算法的最低复杂度，感觉快排已经达到了巅峰。



7/17

#### **学习内容：SQL语言，安装iPython-sql，学习SQL基本架构如row，column，key等。**

SQL基本语句是很熟识的，因为曾在oop大作业时被它折磨的痛不欲生。

索引方法：主要是B+树和哈希

哈希：哈希表 hash（key)->value

B+树：叶节点用链表相连，通过每层的key值寻找目标节点。

索引的意义在于提高操作的效率，

clustered：将内容按索引排序，排好的比随机的快很多。

disk：储存的信息为永久信息，安全可靠，不怕断电。

main memory：程序运行内存，存储临时信息。

SSD：固态，快且安全。

buffer：临时存储，在空闲时大规模导入disk以提高io速度。

桶排，快排，合并排序，外部排序算法：合并两个已排好序的序列，用的空间少

超大文件排序：先快排，然后外部合并排序，直接排序吃不消

B+树的强大：从log2到log200，average fanout：133。



7/18

#### **学习内容：计算机系统 Bitmap索引 机器学习**



*计算机系统=硬件+软件*，

硬件核心：处理器，

系统软件：操统（OS），管理硬件

应用软件：办公软件/数据库

并行分布式系统（多机系统）：超算、大数据

计算机架构：冯诺依曼结构

存储器等级：CPU/RAM/SSD/Disk 磁盘时延高但成本低，

计算机的开发层次 微程序、电子电路（硬件）->机器语言、汇编语言->软件开发、高级语言



*Bitmap索引*

索引：关键字与数据的对应

利用位向量表示集合

大量的0，可进行压缩，但压缩后无法进行随机访问，与或非的运算变得玄学。

位运算速度快，经济性好，编程容易



###### *机器学习*

利用经验集对机器进行训练，提高机器完成任务的性能。

机器学习 = 数据 + 模型 + 算法

Deep Learning

人脑功能：由神经元组成的神经网络实现

人工神经元：用数学建模模拟神经元

激活函数：模拟神经元的激发态

`sigmoid函数`

优点：

值域为（0，1）

###### *网络的拓扑结构*

不同神经元之间的连接关系，前馈（计算图）、反馈

多层前馈网络是一种计算图，隐藏层用于计算

神经网络识别

TensorFlow

各种维度的Tensor

深度学习的本质：数学基础：微积分、线代、概统，试图通过MLP学习函数

训练的本质是找到相应的内部权重，使得实际输出与预期输出差异尽可能小

差异通过度量函数（损失函数或成本函数）来表示

反向传播算法 梯度下降法 随机梯度下降法：初始数据随机化，随机取样本计算权重的偏导数，不断调整权重

批次：整个训练集

迷你批次：批次的子集，

进行完一个迷你批次成为一次迭代，进行完一个批次称为一个时代

Tensor子类：constant，variations，placeholder

输出：权重*输入+偏置







7/19

#### **学习内容：TensorFlow框架，计算图，数据流图，CNN， RNN**

###### *深度学习框架：*

TensorFlow、Pytourch等

计算图

TensorFlow自动生成反向梯度计算图，TPU：矩阵运算加速器

数据流图：

校验点：防止因宕机等因素导致的数据丢失。

TensorFlow架构：

训练库，推断库等

CNN&&RNN

softmax操作把某些结果突出，使得大数更大，小数更小

卷积神经网络关键因素：卷积核的大小和个数，

卷积目标：并行计算、提取特征。

polling：缩小卷积的size，相当于聚合统计。

CNN：用于图片，无时间先后关系

BNN：图灵完备的，用于情感分析，语音识别，文本翻译。



7/20

#### **学习内容： 数据处理**

quantized（模型压缩技术）：在不影响模型精度的情况下减少数据内存

开始训神经网络了，转换 文件格式，下载EXE，augmentation，train，epoch，

一开始出的问题挺多的，但解决之后看到准确率稳步上升还是很开心的，菜鸡也开始训神经网络了

训练最终的准确率是86.09%





7/21

#### **学习内容：论文阅读**

###### A Tutorial on Deep Learning 

机器学习：利用梯度下降的数学方法调节参数使神经网络中的函数误差最小化



随机梯度下降算法

给定一个很小的非负参数数α作为学习频率

随机设置原本参数

随机选取例子

计算偏导数

更新参数值，重复2操作



线性决策函数似乎无法适用于所有的情况？

利用决策函数对数据进行分类，再分别计算决策函数

或者二者不分步进行，而是同时进行，一次性搞定所有参数

反向传播算法：链式法则在神经网络中的特殊应用

利用求导对反向传播debug



参数赋初值是要遵循随机分布或高斯分布，避免对称

还要防止计算结果出现太多的0或者1，即所谓的饱和

通常来说，学习率分布在0.01~0.1之间

过度拟合：层数太多或神经元太多导致完美契合训练集却无法适用于新的情况

低度拟合：与过度拟合相反，两者都是学习率选择不当的表现

双精度有时候并没有存在的必要，单精度对内存更友好

梯度下降时可同时选用多个样本





7/22

#### **学习内容：计算机视觉**

###### 计算机视觉的任务：计算机视觉就是用计算机代替人眼做测量和判断，主要任务包括分类、定位、检测和分割

精确率：判定为正类的正确率

召回率：原样本中正类被判为正的概率

准确率：做出正确判定的概率

识别准确率指标：1、mAP：平均精确率均值   2、PR曲线的覆盖率AUC

IOU：预测框和真实框的重叠联合比



R-CNN 定位目标物，图像分割

输入图像，提取提炼区域

计算CNN



但需要的空间大，时间长

Fast R-CNN

端到端的训练，可独立或联合训练，SVD提速，

Faster R-CNN

主要思想：用最后一个卷积层推断候选区域

RPN给出一系列候选区域，池化层进行分类，提取proposal定位



YOLO

置信度：反映当前边界框存在目标的可能性以及目标位置预测的准确性

缺陷：物体奇形怪状的时候回翻车





anaconda安装血泪史

一、用户名不要用中文，否则只能安装在公用目录下

二、环境变量设置完毕之后重开命令行，否则找不到路径

就这么两个问题，忙活了一个多小时。

好容易安装好了Androidstudio和SDK，但build的时候还是提示中文字符的问题，怕是无解了，有空重装一下window吧



7/23

**学习内容：Haskell语言，学习索引**

函数式语言很神奇

fibonacci挺好写的

试着写写快排什么的，代码太简洁了，四五行就搞定

神经网络代替B树

递归索引模型RM-Index

训练结果一定比B树快，否则直接替换

