# Study Log

by Tang Hao, 2018011281



## Day one(7.15)

1. **In class:**

   Install Jupyter Notebook, Typora

   Insert sort, quick sort, merge sort, big number multiplication

2. **After class:**

   learning how to use Jupyter notebook

   review sort algorithms



//7.19

**insertion sort**

```python
collection = [2,4,3,5,2]

def insert_sort(collection):
    for loop_index in range(1, len(collection)):
        insertion_index=loop_index
        while insertion_index > 0 and collection[insertion_index - 1] > collection[insertion_index]:
            collection[insertion_index], collection[insertion_index - 1] = collection[insertion_index-1], collection[insertion_index]
            insertion_index-=1
    return collection

print(insert_sort(collection))
```

 

**quick sort**

```python
import random

example=[2, 1, 6, 7, 3]

def quick_sort(collection):
    length=len(collection)
    if length <= 1:
        return collection
    else:
        pivot=random.randint(0, len(collection)-1)
        greater=[]
        lesser=[]
        for index in range(0, len(collection)):
            if index != pivot:
                if collection[index] > collection[pivot]:
                    greater.append(collection[index])
                else:
                    lesser.append(collection[index])
        return quick_sort(lesser) + [collection[pivot]] + quick_sort(greater)
print(quick_sort(example))
```



**merge sort**

```python
example = [2, 5, 3, 1, 4, 6, 7]

def merge(left, right):
    merge = []
    while left and right:
        merge.append(left.pop(0) if left[0] <= right[0] else right.pop(0))
    return merge + left + right

def merge_sort(collection):
    left = collection[:len(collection)//2]
    right = collection[len(collection)//2:]
    if len(left) <= 1 and len(right) <= 1:
        return merge(left, right)
    else:
        return merge(merge_sort(left), merge_sort(right))

print(merge_sort(example))
```



## Day Two(7.16)

1. **In class:**

   Bucket Sort, Binary Search Tree, RB Tree, B+ Tree, 2/3/4 Tree, Hashing Table

2. **After class:**

   review, viewing the pdf of the Hashing Table and searching for more about the Hashing Table
   
3. **Concept:**

   **RB Tree:**

   - using two colored-graph to balance binary tree

   - root node must be black
   
   - each node has the same distance to nil
   - note: delete action may 
   
   **B+ Tree & B- Tree:**
   
   - size of nodes: n + 1 pointers and n keys
   
   - for non-leaf node,  use at least [(n + 1) / 2] pointers 
   
   - for leaf node, use at least [(n + 1) / 2]pointers to data
   
   - leaf node should have pointer points to next leaf node	
   
   - note: divide full node before insert action in case the node overflow
   
   - comparison:  B+ Tree allows nodes with same data, B- Tree only allows unique data
   
   **2/3/4 Tree:**
   
   - similar to B Tree
   - can be used to build RB Tree
   
   **Hashing Table:**
   
   - thought: bucket sort like index
   
   - key: hashing function

//7.21

**bucket sort**

```python
#resize DEFAULT_BUCKET_SIZE to change the number of buckets
DEFAULT_BUCKET_SIZE = 5

example = [2, 4, 6, 1, 3, 5, 0, 0, 5, 6, 7]

def bucket_sort(collection, bucket_size = DEFAULT_BUCKET_SIZE):
    min_value, max_value = (min(example), max(example))
    bucket_count = ((max_value - min_value) // bucket_size + 1)
    buckets = [[] for _ in range(int(bucket_count))]

    for i in range(len(collection)):
        buckets[int((collection[i] - min_value) // bucket_size)].append(collection[i])

    return sorted([buckets[i][j]] for i in range(len(buckets))
                                    for j in range(len(buckets[i])))

print(bucket_sort(example))
```



## Day Three(7.17)

1. **In class:**

   Sql, index, storage and memory model

2. **After class:**

   review ppt and the project of database last term
   
3. **Concept:**

   **SQL:** 

   ​	a standard language for querying and manipulating data, high-level programming language

   since I once did a project using SQL, so I will only illustrate the grammar not used in my project

   **FOREIGN KEY:**

   ​	mapping data between different tables

   **Buffer and Disk:**

   - buffer: fast but can't store data

   - disk: able to store data but slow
   
   **Page:**
   
   ​	fixed-sized array of memory
   
   **File:**
   
   ​	variable-length list of pages
   
   **External Merge Algorithm:**
   
   ​	merge the data from disk in buffer



## Day Four(7.18)

1. **In class:**

   Computer system, Memory Hierarchy, Processor, Programming, Bitmap Index for Big Data

   TensorFlow: cognition & perception, machine learning, neural model, activation function, loss function, gradient descent, numerical analysis

   

   //7.19

   softmax (probability vector)

2. **After class:**

   reading PLWAH+: A bitmap index compressing scheme based on PLWAH & ImageNet Classification with Deep Convolutional Neural Networks
   
3. **Concept:**

   **Bitmap:**

   - compaction algorithm for pages(usually compact many 0s)

   **Cognition & Perception:**
   
   - cognition: thinking like human
   
   - perception: act and understand like people
   
   **Machine learning:**
   
   - a research aspect of AI
   
   - using experience set to improve the performance
   
   - procedure: learn - predict- learn
   
   **Deep learning:**
   
   ​	a branch of machine learning
   
   **Generalization error:**
   
   ​	the error between the generalization and the object
   
   **Approximation error:**
   
   ​		lack of the scale of the model
   
   **Estimation error:**
   
   ​		lack of the scale of data set
   
   **Optimization error:**
   
   ​		inefficiency of algorithm	
   
   **Neural model:**
   
   - linear part for reception
   
   - non-linear part for activation
   
   **Activation function:**
   
   ​	sigmod, tanh, ReLU, logistic
   
   **Artificial neural network:**
   
   - feedforward
   
   - feedback
   
   - memory network
   
   **Universal approximators**
   
   **Lost function(cost func):**
   
   ​	sigma or cross entropy
   
   **Backprop:**
   
   ​	calculate gradient(partial derivative) backwards 
   
   **Gradient descent:**
   
   ​	F(x)(usually lost function) declines fastest in the direction of gradient
   
   **Stochastic gradient descent:**
   
   - change the weight to minimize the lost function value
   
   - core: random sample
   
   **Batch, mini-batch, iteration, epoch:**
   
   - batch: the whole training set
   
   - mini-patch: batch divided into mini-batches
   
     - iteration: process of training a mini-batch
   
     - epoch: process of training a batch
   
   **Numerical analysis:**
   
   - method for numerical calculation
   
   - try to minimize the error in training
   
   - principle:
   
   - avoid the absolute value of the divisor is much smaller than the absolute value of dividend
     - avoid the subtraction of two close number
   
   - avoid the lost of accuracy
     - simplify the calculation and reduce the time of  operation calculate
   
   **Differentiation:**
   
   - **Numerical differentiation**
   
   - **Symbolic differentiation**(we use this one in TensorFlow)
   
   - **Automatic differentiation**
   
   **TensorFlow:** using Data Flow Diagram to conduct numerical calculation
   
   - **Graph**
   
   describe the computational graph
   
   - **Session**
   
   do calculation
   
   - **Expression Tree**
   
     - leaf node: operand
   
     - non-leaf node: operator
   
   - **Computational graph**
   
     - leaf node: tensor
   
     - non-leaf node: function
   
   - **Derived class**
   
     - constant: calculate constants
     - variables: parameters in neural network, can be derived in interactions
   
     - placeholder:  input data

## Day Five(7.19)

1. **In class:**

   CNN & RNN

2. **Concept:**

   **Dense**
   
   - multiple layers compose MLP
   - using for classification
   
   **CNN:** convolutional neural network
   
   - **input:** multi-dimensional array
   - **convolution kernel:** a multi-dimensional array, parameter decided by machine learning, usually 32 or 64 size
   - **parameters sharing and pooling**
   - **practical using:** CV, OD
   
   **RNN:** recurrent network
   
   - **parameters sharing during every time step**
   - **practical using:** list problem like speech recognition, handwriting recognition



## Day Six(7.20)

1. **Overview:**

   conduct audioPlot and audioNet with errors to be soluted, posted problem log on saturn-lab github page

   create an instance on Tsinghua AI platform(only accessible in icentre orz.)

2. **Concept:**

   **ASR**(automatic speech recognition)

   - **difficulties:** large scale of words list, tune, noise, emotion ... ...
   - **voice signal:** sound wave and time-varing signal, can be represented as oscillogram, frequency domain graph and time-frequency spectrum
   - **princile formula:** W* = arg max P (W | X)
   - **WER**(word error rate): could exceed 100%
   - **CTC:** predict a sequence of tokens, merge repeats, drop \epsilon, final output
   - **core:** mapping, function
   - **method:** signal - STFFT - continuous voice to small vector - other conversion - vector sequence to voice sequence - voice sequence to character sequence - character sequence to word sequence
   - **pure DNN model**
   - **mix model:** DNN, HMMs, context-dependent phone models, n-gram language model, viterbi search algorithms



## Day Seven(7.21)

**Overview:** review ppt and pdf, complement first week's log



## Day Eight(7.22)

1. **In class:**

   CV

2. **After class:**

   successfully build apk and install, test successfully but low accuracy ... ...

3. **Concept:**

   **CV:** the science (some say art) of programming a computer to process, and ultimately understand, images and video

   - **task:** classification, localization, detection, segmentation
   - **deep neural network**
   - **OD Index**
     - TP: predict positive to positive, FP: predict negative to positive
     - TN: predict negative to negative, FN: predict positive to negative
     - precision(prediction accuracy): TP / (TP + FP)
     - recall(compared to original sample): TP / (TP + FN)
     - accuracy(corrected classified sample / overall sample): (TP+TN)/(TP+FN+FP+TN)

   - **mAP**(mean average precision)
     - the bigger, the more accurate
   - **AUC Index of PR curve**
     - the bigger of AUC, the more accurate
   - **Visual object detection algorithm:** the intersection over union of prediction and ground truth
     - IOU = Area of Intersection / Area of Union
     - correct:
       - IOU > 0.5
     - wrong:
       - location: IOU > 0.1
       - similar: IOU > 0.1
       - other: IOU > 0.1
       - background: IOU < 0.1
   - **Visual detection method**
     - R-CNN(regions with CNN features or region-based CNN)
     - Fast R-CNN
     - Faster R-CNN
     - YOLOv1-->YOLOv3
     - SSD

## Day Nine(7.23)

1. **In class:**

   Haskell, learned sorting, learned index, learned scheduling, learned cardinality estimating

2. **Concept:**

   **Haskell:** a purely functional programming language, totally different thought from programming like language c, python etc.

   **Learned * :** using machine learning method to solve problem, cause neural network can precisely simulate the curve we need, so if we train the network enough, we can reduce the time complexity to O(1)



## Day Ten(7.24)

**Overview:**

- try to deploy starcraft2 as environment, succeed but cannot run the traning model
- succeed to deploy deepfake, succeed but lack of time(cpu may need 1-2 weeks)
- succeed to deploy learning2paint , but failed to train the neural renderer



## Day Elenven(7.25)

**Overview:**

- successfully train the neural renderer, there is a small change in the pillow package, when pillow update to 1.0, function toimage() was removed and cannot be used in pillow >= 1.0, one solution is the degrade the version of pillow to 1.0, but it will not meet the requirement of python version. Another solution is to change the function toimage() to Image.fromarray() to convert an array to a PIL image and convert() to change the mode of the image, in my case, mode F cannot be used in the following steps
- successfully run the easy helmet project but very slow with my cpu