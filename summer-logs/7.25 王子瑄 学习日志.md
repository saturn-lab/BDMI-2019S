7 25 王子瑄 学习日志

大作业分析：

由于GitHub上原作者给出的代码过于混乱，不好理解，因此我采用了CSDN上一位博客给出的优化后的代码（https://blog.csdn.net/songrotek/article/details/50951537）用于分析，感谢原博客的无私奉献！

\1.      训练流程的设计——卷积层、全连接层与Q值层：

​    def createQNetwork(self):

​        \# network weights

​        W_conv1 = self.weight_variable([8,8,4,32])

​        b_conv1 = self.bias_variable([32])

 

​        W_conv2 = self.weight_variable([4,4,32,64])

​        b_conv2 = self.bias_variable([64])

 

​        W_conv3 = self.weight_variable([3,3,64,64])

​        b_conv3 = self.bias_variable([64])

 

​        W_fc1 = self.weight_variable([1600,512])

​        b_fc1 = self.bias_variable([512])

 

​        W_fc2 = self.weight_variable([512,self.ACTION])

​        b_fc2 = self.bias_variable([self.ACTION])

 

​        \# input layer

 

​        self.stateInput = tf.placeholder("float",[None,80,80,4])

 

​        \# hidden layers

​        h_conv1 = tf.nn.relu(self.conv2d(self.stateInput,W_conv1,4) + b_conv1)

​        h_pool1 = self.max_pool_2x2(h_conv1)

 

​        h_conv2 = tf.nn.relu(self.conv2d(h_pool1,W_conv2,2) + b_conv2)

 

​        h_conv3 = tf.nn.relu(self.conv2d(h_conv2,W_conv3,1) + b_conv3)

 

​        h_conv3_flat = tf.reshape(h_conv3,[-1,1600])

​        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat,W_fc1) + b_fc1)

 

​        \# Q Value layer

​        self.QValue = tf.matmul(h_fc1,W_fc2) + b_fc2

 

​        self.actionInput = tf.placeholder("float",[None,self.ACTION])

​        self.yInput = tf.placeholder("float", [None]) 

​        Q_action = tf.reduce_sum(tf.mul(self.QValue, self.actionInput), reduction_indices = 1)

​        self.cost = tf.reduce_mean(tf.square(self.yInput - Q_action))

​        self.trainStep = tf.train.AdamOptimizer(1e-6).minimize(self.cost)

首先是卷积层和全连接层，比较简单，主要是提取状态，也就是图片的特征用于进一步处理。

与CNN不同的是，DQN多了一个Q值层，用来计算Q值：

​            self.QValue = tf.matmul(h_fc1,W_fc2) + b_fc2

这其实就是agent对于当前状态的评估。agent就是根据QValue来决定该状态下的动作的。

再往下就是训练的具体过程，通过Q值和action计算出Q_action，然后和yInput作比较，通过BP算法反向传播以修改前面的参数。

\2.      TargrtQ值的计算：

​    def trainQNetwork(self):

​        \# Step 1: obtain random minibatch from replay memory

​        minibatch = random.sample(self.replayMemory,self.BATCH_SIZE)

​        state_batch = [data[0] for data in minibatch]

​        action_batch = [data[1] for data in minibatch]

​        reward_batch = [data[2] for data in minibatch]

​        nextState_batch = [data[3] for data in minibatch]

 

​        \# Step 2: calculate y 

​        y_batch = []

​        QValue_batch = self.QValue.eval(feed_dict={self.stateInput:nextState_batch})

​        for i in range(0,self.BATCH_SIZE):

​            terminal = minibatch[i][4]

​            if terminal:

​                y_batch.append(reward_batch[i])

​            else:

​                y_batch.append(reward_batch[i] + GAMMA * np.max(QValue_batch[i]))

 

​        self.trainStep.run(feed_dict={

​            self.yInput : y_batch,

​            self.actionInput : action_batch,

​            self.stateInput : state_batch

​            })

该过程主要分为两小步，首先是从记忆模块中读取数据（Step1），然后是计算将要作为y输入的target Q值。

在计算Q值时分了两种情况，若游戏在该状态下停止，那么Q值就是环境给出的reward；若游戏还未停止，说明Q值中除了reward外，还应该包括一个对于未来的期望，也就是：

y_batch.append(reward_batch[i] + GAMMA * np.max(QValue_batch[i]))

​     这里的QValue_batch来自：

​                QValue_batch = self.QValue.eval(feed_dict={self.stateInput:nextState_batch})

​     也即是agent对于下一状态的评估；GAMMAS是一个衰减系数，用于减小误差，因为谁也不能肯定未来的状况一定怎样。

总结：

​        基于DQN的flappy bird模型，我认为本质上就是state到行为的映射，电脑通过记忆中的例子进行学习，将该映射的各个参数调整为最佳状态，从而实现对于任意一步都能够做出合理的反应。