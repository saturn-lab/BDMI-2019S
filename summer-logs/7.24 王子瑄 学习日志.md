7 24 王子瑄 学习日志

选题：

基于DQN的flappy bird：

2013年DeepMind 在NIPS上发表Playing Atari with Deep Reinforcement Learning 一文，提出了DQN（Deep Q Network）算法，实现端到端学习玩Atari游戏，即只有像素输入，看着屏幕玩游戏。

Flappy Bird是个极其简单又困难的游戏，风靡一时。在很早之前，就有人使用Q-Learning 算法来实现完Flappy Bird——http://sarvagyavaish.github.io/FlappyBirdRL/ 但是这个实现是通过获取小鸟的具体位置信息来实现的。

那么，能否使用DQN来实现通过屏幕学习玩Flappy Bird呢？

在github上，有人给出了使用DQN玩Flappy Bird的代码，成功实现了这一模型。

基本原理：

DQN是增强学习（Reinforcement Learning）的一种扩展。增强学习的逻辑图如上：在某一时刻，agent接受了环境的一个状态（state），并对此状态进行评估，做出一个相应的动作（action）。之后，环境通过结合状态与动作给出一个奖赏值（reward），反馈给agent。接着，agent根据以往的动作状态集合来修改自己决策中的各项参数，从而实现强化学习。

DQN的本质是将深度学习和增强学习结合在一起后形成的一个深度增强学习网络。对于flappy bird来说，agent首先根据当前环境给出的图片来预测Q值（初始化时可能是随机的），也就是判断当前状态的好坏程度——马上要碰到障碍还是成功越过障碍。然后，根据Q值判断出一个动作，再接受环境由此状态和动作给出的奖赏值，将其记忆，作为接下来训练的一个实例。在训练时，agent通过将某一实例的奖赏值转化为target Q值输入（Optimal value function 最优动作估值函数），作为一次训练的标签值y，而正如前面所说，agent在接受状态后输出的其实是一个Q值，一个对环境的评价。因此，训练的过程实质上是将输出Q值尽可能地接近target Q值，也就是通过学习让agent尽可能地对环境给出的状态做出合理的评价，从而给出最佳动作（跳还是不跳？）