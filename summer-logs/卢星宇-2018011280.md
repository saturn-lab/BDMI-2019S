卢星宇-2018011280 



7/15

#### **学习内容：安装jupyter notebook，设置git路径，下载课件，初识Python语法，排序 大数乘法**

路径是已经设置好的，插入排序、合并排序，快排以及桶排序是已经熟识的排序算法，也用C++实现过，不知用Python实现会怎么样。复习一下很好，正好多多阅读Python代码。jupyter notebook好像是一种远程桌面，功能才是很强大的，但得一直占着cmd 最有收获的是大数乘法算法的不断改进以及课后那篇最佳算法的推送



7/16

#### **学习内容：红黑树，二叉搜索树，2-3-4树，B+树，哈希查找**。

感觉最奇怪的是红黑树，节点颜色的判定成谜，插入和删除操作有点复杂，好在logn的复杂度很能令人满意，以及完美的平衡效果。2-3-4树是比较陌生的，从来没想过可以在节点里放多个key值，人类的智慧可谓伟大。LLRB树区别于RB树，对边而不是对节点染色，虽然就平衡效果来说，2-3-4树和LLRB树逊色于红黑，但胜在实现方便。今天学到的一个证明是排序算法的最低复杂度，感觉快排已经达到了巅峰。



7/17

#### **学习内容：SQL语言，安装iPython-sql，学习SQL基本架构如row，column，key等。**

SQL基本语句是很熟识的，因为曾在oop大作业时被它折磨的痛不欲生。

索引方法：主要是B+树和哈希

哈希：哈希表 hash（key)->value

B+树：叶节点用链表相连，通过每层的key值寻找目标节点。

索引的意义在于提高操作的效率，

clustered：将内容按索引排序，排好的比随机的快很多。

disk：储存的信息为永久信息，安全可靠，不怕断电。

main memory：程序运行内存，存储临时信息。

SSD：固态，快且安全。

buffer：临时存储，在空闲时大规模导入disk以提高io速度。

桶排，快排，合并排序，外部排序算法：合并两个已排好序的序列，用的空间少

超大文件排序：先快排，然后外部合并排序，直接排序吃不消

B+树的强大：从log2到log200，average fanout：133。



7/18

#### **学习内容：计算机系统 Bitmap索引 机器学习**



*计算机系统=硬件+软件*，

硬件核心：处理器，

系统软件：操统（OS），管理硬件

应用软件：办公软件/数据库

并行分布式系统（多机系统）：超算、大数据

计算机架构：冯诺依曼结构

存储器等级：CPU/RAM/SSD/Disk 磁盘时延高但成本低，

计算机的开发层次 微程序、电子电路（硬件）->机器语言、汇编语言->软件开发、高级语言



*Bitmap索引*

索引：关键字与数据的对应

利用位向量表示集合

大量的0，可进行压缩，但压缩后无法进行随机访问，与或非的运算变得玄学。

位运算速度快，经济性好，编程容易



###### *机器学习*

利用经验集对机器进行训练，提高机器完成任务的性能。

机器学习 = 数据 + 模型 + 算法

Deep Learning

人脑功能：由神经元组成的神经网络实现

人工神经元：用数学建模模拟神经元

激活函数：模拟神经元的激发态

`sigmoid函数`

优点：

值域为（0，1）

###### *网络的拓扑结构*

不同神经元之间的连接关系，前馈（计算图）、反馈

多层前馈网络是一种计算图，隐藏层用于计算

神经网络识别

TensorFlow

各种维度的Tensor

深度学习的本质：数学基础：微积分、线代、概统，试图通过MLP学习函数

训练的本质是找到相应的内部权重，使得实际输出与预期输出差异尽可能小

差异通过度量函数（损失函数或成本函数）来表示

反向传播算法 梯度下降法 随机梯度下降法：初始数据随机化，随机取样本计算权重的偏导数，不断调整权重

批次：整个训练集

迷你批次：批次的子集，

进行完一个迷你批次成为一次迭代，进行完一个批次称为一个时代

Tensor子类：constant，variations，placeholder

输出：权重*输入+偏置







7/19

#### **学习内容：TensorFlow框架，计算图，数据流图，CNN， RNN**

###### *深度学习框架：*

TensorFlow、Pytourch等

计算图

TensorFlow自动生成反向梯度计算图，TPU：矩阵运算加速器

数据流图：

校验点：防止因宕机等因素导致的数据丢失。

TensorFlow架构：

训练库，推断库等

CNN&&RNN

softmax操作把某些结果突出，使得大数更大，小数更小

卷积神经网络关键因素：卷积核的大小和个数，

卷积目标：并行计算、提取特征。

polling：缩小卷积的size，相当于聚合统计。

CNN：用于图片，无时间先后关系

BNN：图灵完备的，用于情感分析，语音识别，文本翻译。



7/20

#### **学习内容： 数据处理**

quantized（模型压缩技术）：在不影响模型精度的情况下减少数据内存

开始训神经网络了，转换 文件格式，下载EXE，augmentation，train，epoch，

一开始出的问题挺多的，但解决之后看到准确率稳步上升还是很开心的，菜鸡也开始训神经网络了

训练最终的准确率是86.09%





7/21

#### **学习内容：论文阅读**

###### A Tutorial on Deep Learning 

机器学习：利用梯度下降的数学方法调节参数使神经网络中的函数误差最小化



随机梯度下降算法

给定一个很小的非负参数数α作为学习频率

随机设置原本参数

随机选取例子

计算偏导数

更新参数值，重复2操作



线性决策函数似乎无法适用于所有的情况？

利用决策函数对数据进行分类，再分别计算决策函数

或者二者不分步进行，而是同时进行，一次性搞定所有参数

反向传播算法：链式法则在神经网络中的特殊应用

利用求导对反向传播debug



参数赋初值是要遵循随机分布或高斯分布，避免对称

还要防止计算结果出现太多的0或者1，即所谓的饱和

通常来说，学习率分布在0.01~0.1之间

过度拟合：层数太多或神经元太多导致完美契合训练集却无法适用于新的情况

低度拟合：与过度拟合相反，两者都是学习率选择不当的表现

双精度有时候并没有存在的必要，单精度对内存更友好

梯度下降时可同时选用多个样本





7/22

#### **学习内容：计算机视觉**

###### 计算机视觉的任务：计算机视觉就是用计算机代替人眼做测量和判断，主要任务包括分类、定位、检测和分割

精确率：判定为正类的正确率

召回率：原样本中正类被判为正的概率

准确率：做出正确判定的概率

识别准确率指标：1、mAP：平均精确率均值   2、PR曲线的覆盖率AUC

IOU：预测框和真实框的重叠联合比



R-CNN 定位目标物，图像分割

输入图像，提取提炼区域

计算CNN



但需要的空间大，时间长

Fast R-CNN

端到端的训练，可独立或联合训练，SVD提速，

Faster R-CNN

主要思想：用最后一个卷积层推断候选区域

RPN给出一系列候选区域，池化层进行分类，提取proposal定位



YOLO

置信度：反映当前边界框存在目标的可能性以及目标位置预测的准确性

缺陷：物体奇形怪状的时候回翻车





anaconda安装血泪史

一、用户名不要用中文，否则只能安装在公用目录下

二、环境变量设置完毕之后重开命令行，否则找不到路径

就这么两个问题，忙活了一个多小时。

好容易安装好了Androidstudio和SDK，但build的时候还是提示中文字符的问题，怕是无解了，有空重装一下window吧



7/23

#### **学习内容：Haskell语言，学习索引**

函数式语言很神奇

fibonacci挺好写的

试着写写快排什么的，代码太简洁了，四五行就搞定

神经网络代替B树

递归索引模型RM-Index

训练结果一定比B树快，否则直接替换



7/24

#### 学习内容：阅读论文，准备大作业

我选的论文是An O(N) Sorting Algorithm: Machine Learning Sort ，一种复杂度为O(n)的机器学习排序算法，起初是被这个名字吸引，毕竟能达到O(n)的复杂度还是很诱人的。

这篇文章的一路大致是，利用海量数据中的一部分对机器学习模型——一个三层神经网络进行训练，得到整个数据集的数据分布情况，然后直接依据分布情况和主值进行粗排，然后对粗排得到的序列进行微调，修正部分错误。这样的复杂度为O(m*n)其中m为神经网络中神经元的个数，其理论下界为1。

从作者提供的实验数据来看，排序耗时与数据量严格成正比，且比大多数算法要快，仅略次于高度优化的numpy的快排，在数据分布不规则的情况下也有很好的鲁棒性，可以保证每个间隔中只有很少的数据量

这一算法的优势在于TPU和GPU的广阔加速空间以及并行计算之后时耗的降低。

但这一算法存在的问题有

​	1、有可能过度拟合

​	2、理论下界实际上不可能达到，即只有面对规模极大的数据，该算法才有优势

​	3、神经网络得到的概率分布并不精确，当一个间隔中有多个数字时，该模型有可能会崩溃

网上对这篇文章的风评并不好，有人说作者对算法复杂度缺乏基础的认知，我想这倒不至于，但感觉作者讲的算法很像邓公讲的一种算法：在数据分布较为有规律的情况下的排序优化算法，而这个算法正好把先前设定好的数据分布工作交给机器学习。其方法高下我不得而知。



7/25

#### 学习内容：读论文，参观声智，试着跑安全帽项目

今天一开始还算舒适，早上啃了一上午的论文，回到寝室室友还没起床，“何时得遂田园乐，睡到人间饭熟时”，真令人羡慕。

论文不是很好懂，很多概念我都不清楚。具体内容以后再谈。

下午参观声智，模型很好玩，原理也很有趣，挺可惜的是没听到关于NLP原理方面的知识，前面信号处理方面毕竟用到机器学习的还很少。但声智能发展的这么快还是令人钦佩的。

晚上是一天噩梦的开始，训练模型嘛，当然要用linux系统，没说的，切manjaro，然后，一个compiler死活安装不上，万般无奈，从磁盘分区开始学装ubuntu，奈何技术不到家，装完之后找不到ubuntu不说，启动盘还坏了，以后开Windows都要手动操作。举目无亲，只能拿VBox碰碰运气，用了15分钟我就放弃了，太慢了，打开文件夹都困难，怎么能指望它训神经网络呢？

十一点的时候我才想到装WSL，然后一切开始有条不紊，直到检查环境的时候发现找不到Helmet文件，算了，已经一点半了，明天再调路径吧。



7/26

#### 学习内容：阅读论文，准备大作业，试图使用服务器

今天是真的难受，早上把步骤重跑了一遍，终于可以开始训练了，但别人的都四五秒一次，为什么我的要十几秒？跑到了中午十二点多才跑了600多步，看来不得不试试用服务器了。

先是网络，怎么连上网就花了两个小时，登上去之后在下载文件之前一切都正常，直到我把三个来路不明的文件复制到了train目录下，然后试图开始训练，结果当然是失败了，成功训练之后发现服务器CPU比本子快不了多少，想装GPU，可已经三点多了，再不把论文看完PPT就没时间做了。我只能说我尽力了，先看论文吧，加分还是随缘就好。

**论文**

SageDB: A Learned Database System 

一种利用机器学习全新的生成数据库的方法

通用设计的弊端

键为连续整数，本身可用作偏移量，则无B+树的使用必要，排序可优化为O(n)

一个C程序，可以在笔记本上用300 毫秒对100亿个整数进行求和，但在现代数据库上执行相同的操作需要150秒，

专门为某个实例设计一种系统是不值得的，但如果能够学习数据的模式相关性，则可以自动综合索引结构，排序和连接算法甚至整个查询优化器，提高性能

学习型组件可以代替数据库系统的核心组件，如索引结构，排序和连接算法甚至整个查询优化器，

如何提供传统方法的语义保证？评估模型的成本怎么办？

性能方面：硬件的发展趋势使得通过神经网络代替重分支的算法前景光明

语义保证：B树：含有记录特定键的页面的模型，用执行性能换取准确性，而学习模型在换取的权衡方面拥有更好的灵活性。

构建SageDB：彻底改变目前开发数据库的方式，定制数据库

定制的方式：

针对配置：找到系统的最佳配置，关于这方面的自动调优已经有许多的工作   

针对算法：查询优化器决定最佳执行顺序和最佳实现方法

自体设计：探索关键步骤的可能性，自动生成工作负载和硬件的最佳设计

通过学习来定制：用学习模型来替换数据系统核心组件，将计算类型从传统的控制流密集型计算转为数据依赖型计算，可以在CPU和ml加速器上更有效地执行。最大的优点：模型是可以共享的。但为了保证正确性，必须与传统算法和数据结构相结合。

 

实例：假设有一个完美的经验累计分布函数模型

​       优化器：关于最佳连接次序的决策变得简单

​       索引：模型支持点和范围索引，通过小于某个主键的概率计算偏移量，使得任何查找都是O(1)操作

​       压缩：通过反转经验累积分布函数模型，就可以利用列的位置来对应值，从而避免对值得存储。

​       执行操作：排序：根据值预测位置，将排序复杂度降为O(n)

模型的选择：管用就行

一维RMI模型：通过上一层模型的预测结果划分子区域，选择下一个模型，直到最后一个模型做出最终预测，显著优于索引结构，且易于训练。

 

如何代替索引：

​       一维索引：利用模型进行排序，然后对预测结果进行本地搜索。使用RMI模型可比最先进的B树快2倍

​       多维索引和存储布局：最大的困难在于纠正模型不精确的预测结果，把点排序并划分为单元格，划分方式通过数据和查询分布自动学习。结论：在每种查询类型上，学习索引的性能都优于集群索引

 

排序的加速方法

一种复杂度为O(n)的机器学习排序算法。粗排，润色，合并。

合并操作：可以根据已有的概率分布模型跳过部分重合度不高的数据

调度的优化：传统方法：先到先得，SageDB：根据数据和工作负载做出调度决策。


7/27

#### **学习内容：同学们的大作业展示，自己跑Helmet项目**

今天早上的行业介绍不是很对胃口，大概学长也向早点结束，有点敷衍的感觉。

然后是同学们的大作业展示，内容之多让人眼花缭乱，课件同学们是下了功夫的，印象比较深的有汤昊的照片->油画，李晓雯的图片渲染，以及一位同学的诗词。

直到晚上，终于填完了所有的坑，开始在WSL上通过服务器跑Helmet项目，看着一步只需要0.5秒的飞快的训练速度，心里还是很高兴的，感觉之前填坑的血泪史都值了。跑到3万多步，我该睡了，节课开心！
